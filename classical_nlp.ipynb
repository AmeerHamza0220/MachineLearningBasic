{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled21.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOCmp+eDI1njxzSw3OYuNRI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmeerHamza0220/MachineLearningBasic/blob/main/classical_nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlyIbvF4QkzR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd # provide sql-like data manipulation tools. very handy.\n",
        "pd.options.mode.chained_assignment = None\n",
        "import numpy as np # high dimensional vector computing library.\n",
        "from copy import deepcopy\n",
        "from string import punctuation\n",
        "from random import shuffle\n",
        "\n",
        "import gensim\n",
        "from gensim.models.word2vec import Word2Vec # the word2vec model gensim class\n",
        "LabeledSentence = gensim.models.doc2vec.LabeledSentence # we'll talk about this down below\n",
        "\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas(desc=\"progress-bar\")\n",
        "\n",
        "from nltk.tokenize import TweetTokenizer # a tweet tokenizer from nltk.\n",
        "tokenizer = TweetTokenizer()\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/vineetdhanawat/twitter-sentiment-analysis/master/datasets/Sentiment%20Analysis%20Dataset.csv"
      ],
      "metadata": {
        "id": "ibnJU57zS-0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=pd.read_csv('Sentiment Analysis Dataset.csv',encoding='ISO-8859-1')\n",
        "data"
      ],
      "metadata": {
        "id": "lzuihA9-TBte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.drop(['ItemID'], axis=1, inplace=True)\n",
        "data = data[data.Sentiment.isnull() == False]\n",
        "data['Sentiment'] = data['Sentiment'].map(int)\n",
        "data = data[data['SentimentText'].isnull() == False]\n",
        "data.reset_index(inplace=True)\n",
        "data.drop('index', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "ACNrrDICVPmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head(5)"
      ],
      "metadata": {
        "id": "HE_v9POKVfRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(tweet):\n",
        "    try:\n",
        "        tokens = tokenizer.tokenize(tweet)\n",
        "        return tokens\n",
        "    except:\n",
        "        return 'NC'\n",
        "\n",
        "data['tokens'] = data['SentimentText'].progress_map(tokenize)"
      ],
      "metadata": {
        "id": "5Li7QmtgVqFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "t6VRGPWmWKgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(np.array(data['tokens']),\n",
        "                                                    np.array(data['Sentiment']), test_size=0.2)"
      ],
      "metadata": {
        "id": "_pzR_Ri8Xl80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer \n",
        "import gensim"
      ],
      "metadata": {
        "id": "SHme95rUbBmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = ' '.join([text for text in data['SentimentText']]) \n",
        "\n",
        "from wordcloud import WordCloud\n",
        "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words) \n",
        "plt.figure(figsize=(10, 7)) \n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\") \n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tCI-uzAnb_1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10, 7)) \n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\") \n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JfAjRNp9ciQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bag-of-Words Features.\n",
        "bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
        "bow = bow_vectorizer.fit_transform(data['SentimentText'])\n",
        "bow.shape"
      ],
      "metadata": {
        "id": "Oe8rXrl8bZVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
        "tfidf = tfidf_vectorizer.fit_transform(data['SentimentText'])\n",
        "tfidf.shape"
      ],
      "metadata": {
        "id": "e-Z8hV7Rc_fL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import multiprocessing\n",
        "\n",
        "cores= multiprocessing.cpu_count()\n",
        "cores"
      ],
      "metadata": {
        "id": "P6TuZu7euMD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "model_w2v =Word2Vec(\n",
        "            size=200, # desired no. of features/independent variables\n",
        ") \n"
      ],
      "metadata": {
        "id": "unzBy0ksdUOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_w2v.build_vocab([i for i in data['tokens']])\n"
      ],
      "metadata": {
        "id": "jA7ZmyS9xKI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tqdm()"
      ],
      "metadata": {
        "id": "t9T3CVWJx8-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_w2v.train(data['tokens'], total_examples= len(data['SentimentText']), epochs=4)"
      ],
      "metadata": {
        "id": "4vOvvqZswabR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score"
      ],
      "metadata": {
        "id": "RF3Bxn4UwBQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_bow = bow[:31962,:] \n",
        "test_bow = bow[31962:,:] \n",
        "\n",
        "# splitting data into training and validation set \n",
        "xtrain_bow, xvalid_bow, ytrain, yvalid = train_test_split(train_bow, data['Sentiment'][:31962], random_state=42, test_size=0.3)\n",
        "\n",
        "lreg = LogisticRegression(solver='lbfgs') \n",
        "\n",
        "# training the model \n",
        "lreg.fit(xtrain_bow, ytrain) \n",
        "prediction = lreg.predict_proba(xvalid_bow) # predicting on the validation set \n",
        "prediction_int = prediction[:,1] >= 0.3 # if prediction is greater than or equal to 0.3 than 1 else 0 \n",
        "prediction_int = prediction_int.astype(np.int) \n",
        "f1_score(yvalid, prediction_int) # calculating f1 score for the validation set"
      ],
      "metadata": {
        "id": "aLabNCdazFPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_tfidf = tfidf[:31962,:]\n",
        "test_tfidf = tfidf[31962:,:] \n",
        "\n",
        "xtrain_tfidf = train_tfidf[ytrain.index]\n",
        "xvalid_tfidf = train_tfidf[yvalid.index]\n",
        "\n",
        "lreg.fit(xtrain_tfidf, ytrain) \n",
        "\n",
        "prediction = lreg.predict_proba(xvalid_tfidf)\n",
        "\n",
        "prediction_int = prediction[:,1] >= 0.3\n",
        "prediction_int = prediction_int.astype(np.int) \n",
        "\n",
        "f1_score(yvalid, prediction_int)"
      ],
      "metadata": {
        "id": "ItYPbdIvz9Jp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_w2v = wordvec_df.iloc[:31962,:]\n",
        "test_w2v = wordvec_df.iloc[31962:,:]\n",
        "\n",
        "xtrain_w2v = train_w2v.iloc[ytrain.index,:]\n",
        "xvalid_w2v = train_w2v.iloc[yvalid.index,:]\n",
        "\n",
        "lreg.fit(xtrain_w2v, ytrain) \n",
        "\n",
        "prediction = lreg.predict_proba(xvalid_w2v)\n",
        "\n",
        "prediction_int = prediction[:,1] >= 0.3\n",
        "prediction_int = prediction_int.astype(np.int)\n",
        "\n",
        "f1_score(yvalid, prediction_int)"
      ],
      "metadata": {
        "id": "QKycDh7z0Ce4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}