{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled37.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyP0Vze7+GjeydrcGxHOhs6W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    },
    "language_info": {
      "name": "R"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmeerHamza0220/MachineLearningBasic/blob/main/Untitled37.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Cross-Validation</b>\n",
        "In the absence of a very large designated test set that can be used to\n",
        "directly estimate the test error rate, a number of techniques can be used to estimate this quantity using the available training data. Some methods make a mathematical adjustment to the training error rate in order to estimate the test error rate. Such approaches are discussed in Chapter 6. In this section, we instead consider a class of methods that estimate the test error rate by holding out a subset of the training observations from the fitting process, and then applying the statistical learning method to those held out observations.\n",
        "\n",
        "<b>The Validation Set Approach</b>\n",
        "A set of n observations are randomly split into a training set (shown in blue, containing observations 7, 22, and 13, among others) and a validation set (shown in beige, and containing observation 91, among others). The statistical learning method is fit on the training set, and its performance is evaluated on the validation set.\n",
        "\n",
        "<b>Leave-One-Out Cross-Validation</b>\n",
        "instead of creating two subsets of comparable size, a single observation (x1,y1) is used for the validation set, and the remaining observations {(x2,y2),. .., (xn,yn)} make up the training set. The statistical learning method is fit on the n − 1 training observations, and a prediction ˆy1 is made for the excluded observation\n",
        "\n",
        "<b>k-Fold Cross-Validation</b>\n",
        "An alternative to LOOCV is k-fold CV. This approach involves randomly\n",
        "dividing the set of observations into k groups, or folds, of approximately equal size. TA schematic display of 5-fold CV. A set of n observations is randomly split into five non-overlapping groups. Each of these fifths acts as a validation set (shown in beige), and the remainder as a training set (shown in blue). The test error is estimated by averaging the five resulting MSE estimates\n",
        "\n",
        "<b>The Bootstrap</b>\n",
        "The bootstrap is a widely applicable and extremely powerful statistical tool\n",
        "that can be used to quantify the uncertainty associated with a given estimator or statistical learning method\n",
        "the bootstrap approach allows us to use a computer to emulate the process of obtaining new sample sets, so that we can estimate the variability of ˆα without generating additional samples. Rather than repeatedly obtaining independent data sets from the population, we instead obtain distinct data sets by repeatedly sampling observations from the original data set.\n",
        "bootstrap approach on a small sample containing n =3 observations. Each bootstrap data set contains n observations, sampled with replacement from the original data set. Each bootstrap data set is used to obtain an estimate of α."
      ],
      "metadata": {
        "id": "3QDBe4dVwP0_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7iylv_sYwOvD"
      },
      "outputs": [],
      "source": [
        "install.packages('ISLR2')\n",
        "\n",
        "library(ISLR2)\n",
        "attach(Auto)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this lab, we will perform linear regression using the glm() function rather than the lm() function because the former can be used together with cv.glm(). The cv.glm() function is part of the boot library."
      ],
      "metadata": {
        "id": "JXDvbHQUwt57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "library(boot)"
      ],
      "metadata": {
        "id": "IPIEJ77SwerL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "glm.fit=glm(mpg~horsepower,data=Auto)\n",
        "cv.arr=cv.glm(Auto,glm.fit)\n",
        "cv.arr$delta"
      ],
      "metadata": {
        "id": "XK8ZxyM9wyDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The two numbers in the delta vector contain the cross-validation results. In this case the numbers are identical (up to two decimal places) and correspond to the LOOCV statistic given in (5.1). Below, we discuss a situation in which the two numbers differ. Our cross-validation estimate for the test error is approximately 24.23. We can repeat this procedure for increasingly complex polynomial fits. To automate the process, we use the for() function to initiate a for loop\n",
        "which iteratively fits polynomial regressions for polynomials of order i =1 to i = 10, computes the associated cross-validation error, and stores it in the ith element of the vector cv.error."
      ],
      "metadata": {
        "id": "e66Kw5YFxDlk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cv.error=rep(0,10)\n",
        "degree=1:10\n",
        "for (i in degree){\n",
        "    glm.fit=glm(mpg~poly(horsepower,i),data=Auto)\n",
        "    cv.error[i]=cv.glm(Auto,glm.fit)$delta\n",
        "\n",
        "}\n",
        "cv.error"
      ],
      "metadata": {
        "id": "_AejLDCGxEZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "using our own function using formula (5.2)"
      ],
      "metadata": {
        "id": "ZvficgZ5xwz7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loocv=function(fit){\n",
        "    h=lm.influence(fit)$h\n",
        "    mean((residuals(fit)/(1-h))^2)\n",
        "}"
      ],
      "metadata": {
        "id": "BpbkIKoJxy2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#this is faster\n",
        "for (i in degree){\n",
        "    glm.fit=glm(mpg~poly(horsepower,i),data=Auto)\n",
        "    cv.error[i]=loocv(glm.fit)\n",
        "\n",
        "}\n",
        "cv.error"
      ],
      "metadata": {
        "id": "JROjJCWeyY4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot(degree,cv.error,type='b')"
      ],
      "metadata": {
        "id": "GY9cxsfQyscd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}