{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled23.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMKWDP0KYdCt+/mIYh9EDel",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmeerHamza0220/MachineLearningBasic/blob/main/nlp_getting_starting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3oAUPG3klv6k"
      },
      "outputs": [],
      "source": [
        "! pip install -q kaggle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "ykiq2kt8mype"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle\n",
        "\n",
        "! cp kaggle.json ~/.kaggle/"
      ],
      "metadata": {
        "id": "5kxTUixOm5gt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xYprSzGSn360"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "S_Rj0QUPnH7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --force-reinstall --no-deps kaggle"
      ],
      "metadata": {
        "id": "RzZ5HVATn4oz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle competitions download -c nlp-getting-started"
      ],
      "metadata": {
        "id": "_E1ZFtiGm86T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip nlp-getting-started.zip"
      ],
      "metadata": {
        "id": "t6fLwWu3oZZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gensim\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "from tqdm import tqdm\n",
        "from nltk.tokenize import TweetTokenizer # a tweet tokenizer from nltk.\n",
        "tokenizer = TweetTokenizer()\n",
        "tqdm.pandas(desc=\"progress-bar\")\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "OdTuHA-zogjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=pd.read_csv(\"train.csv\")\n",
        "data"
      ],
      "metadata": {
        "id": "VRVzuVX2o4lr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(tweet):\n",
        "    try:\n",
        "        tokens = tokenizer.tokenize(tweet)\n",
        "        return tokens\n",
        "    except:\n",
        "        return 'NC'"
      ],
      "metadata": {
        "id": "HrSDYXi0pk7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['tokens']=data['text'].progress_map(tokenize)"
      ],
      "metadata": {
        "id": "J8IR0gUXpY_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "J2Heaokvpvqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer \n",
        "bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
        "bow = bow_vectorizer.fit_transform(data['text'])\n"
      ],
      "metadata": {
        "id": "ELGZwPiqp1lJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "log_reg = LogisticRegression(solver='lbfgs') \n",
        "log_reg.fit(bow,data['target'])\n",
        "log_reg.score(bow,data['target'])"
      ],
      "metadata": {
        "id": "ihMUET6oqMy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfid=TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
        "tfid_x=tfid.fit_transform(data['text'])\n",
        "log_reg.fit(tfid_x,data['target'])\n",
        "log_reg.score(tfid_x,data['target'])"
      ],
      "metadata": {
        "id": "CbCwYQ7EqpHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v =Word2Vec(\n",
        "            size=200, # desired no. of features/independent variables\n",
        ")\n",
        "w2v.build_vocab([i for i in tqdm(data['tokens'])])"
      ],
      "metadata": {
        "id": "7WbGOWn8rFgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v.train(data['tokens'], total_examples= len(data['text']), epochs=4)"
      ],
      "metadata": {
        "id": "zzYeJ1igrXL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v.most_similar('killed')"
      ],
      "metadata": {
        "id": "3yCkXAYKra0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def word_vector(tokens, size):\n",
        "    vec = np.zeros(size).reshape((1, size))\n",
        "    count = 0\n",
        "    for word in tokens:\n",
        "        try:\n",
        "            vec += w2v[word].reshape((1, size))\n",
        "            count += 1.\n",
        "        except KeyError:  # handling the case where the token is not in vocabulary\n",
        "            continue\n",
        "    if count != 0:\n",
        "        vec /= count\n",
        "    return vec\n",
        "\n",
        "wordvec_arrays = np.zeros((len(data['tokens']), 200)) \n",
        "for i in range(len(data['tokens'])):\n",
        "    wordvec_arrays[i,:] = word_vector(data['tokens'][i], 200)\n",
        "wordvec_df = pd.DataFrame(wordvec_arrays)\n",
        "wordvec_df.shape"
      ],
      "metadata": {
        "id": "4vZU2R8XseW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_reg.fit(wordvec_df,data['target'])\n",
        "log_reg.score(wordvec_df,data['target'])"
      ],
      "metadata": {
        "id": "UeOVhCpptFh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bow.shape"
      ],
      "metadata": {
        "id": "mA_2Iaustw2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Dense(100, activation='tanh', input_dim=200))\n",
        "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(wordvec_arrays, data['target'], epochs=9, batch_size=32, verbose=2)"
      ],
      "metadata": {
        "id": "miBlyQmBrqjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "U0nO0BLqsddK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}