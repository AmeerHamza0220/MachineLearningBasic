{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled38.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMtVTfZA3sJAbbh26opVdjS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    },
    "language_info": {
      "name": "R"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmeerHamza0220/MachineLearningBasic/blob/main/ModelSelectionR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By removing extra features our model becomes more interpretable.\n",
        "We will see.\n",
        "We will see following approach for feature selection\n",
        "<li>Subset selection</li>\n",
        "We try all combination of subset and select best\n",
        "<li>Shrinkage</li>\n",
        "Ridge regression and lasso are examples\n",
        "<li>Dimention reduction</li>\n",
        "Linear combination and projection\n",
        "\n",
        "<b>Subset selection</b>\n",
        "\n",
        "First we create null model $M_0$\n",
        "Then \n",
        "For k from 1 to p:\n",
        " Fit all ${p\\choose k}$ choose k model select best save it to $M_k$\n",
        "\n",
        "Finally we need to select the best model from  $M_0$ to $M_k$. There are many ways to do that.\n",
        "NOTE: Subset selection cannot ne applied when p is large because there are $2^p$ total possible predictors. And it also suffer from overfitting.\n",
        "\n",
        "<b>Stepwise selection</b>\n",
        "Instead of fitting all possible predictor we fit on #p^2$.\n",
        "Start with $M_0$ \n",
        "for all k from 0 to p-1:\n",
        " select all p-k model\n",
        " select best\n",
        "\n",
        " Backward selection is opposite. Start with $M_p$ then remove predictor one at a time"
      ],
      "metadata": {
        "id": "Nlde7jKj8F_P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will see some way to select best M\n",
        "\n",
        "$C_p$ choose model with smallest $C_p$\n",
        "\n",
        "AIC criteria: choose model with smallest AIC since AIC is proportional\n",
        "\n",
        "BIC similar AIC but put larger penalty for larger model\n",
        "\n",
        "Adjusted $R^2$\n",
        "Big $R^2$ tell that model fit the data well but we cannot use $R^2$ to compare model with different #predictors.\n",
        "Adjusted $R^2$ fix that by penalizing large model so number are comparable.\n",
        "\n",
        "**Cross validation** It is better then other approaches and we also don't need to estimate $σ^2$ \n",
        "\n",
        "**Shrinkage **\n",
        "\n",
        "**Ridge Regression**\n",
        "\n",
        "In ordinary least square we minimize RSS in contrast ridge regression minimize \n",
        "$RSS+λ\\sum_{j=0}^{p}β_j^p$\n",
        "\n",
        "We are penalizing the coefficient. λ is the amount of penality. We are shrinking the parameter toward 0.\n",
        "Before applying the ridge regression we standarlize the predictors by divide each feature by their standard deviation using\n",
        "$x̂_{ij}= \\frac{x_{ij}}{\\sqrt{\\frac{1}{n}\\sum_{i=1}{n}(x_{ij}-x̂_j)}}$\n",
        "\n"
      ],
      "metadata": {
        "id": "g7g0aXkjHyMA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lasso**\n",
        "\n",
        "Drawback of ridge regression is that it will select all p predictors.\n",
        "In lasso minimize:\n",
        "$RSS+λ\\sum_{j=0}^{p}|β_j|$\n",
        "\n",
        "This is called L1 penality. It does subset selection. Lasso model called sparcity model.\n",
        "\n",
        "**Tuning parameter**\n",
        "\n",
        "We use cross validation for tuning λ."
      ],
      "metadata": {
        "id": "uEgTFekaTc40"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2r_jf3f8FHj"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ]
}